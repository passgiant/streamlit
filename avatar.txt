# st_chatbot.py
import google.generativeai as genai 
import streamlit as st
from dotenv import load_dotenv
import os

# Create the model
# ëª¨ë¸ì˜ ì„¤ì •
generation_config = {
    # ì˜¨ë„ê°€ ì˜¬ë¼ê°€ë©´ ì˜ˆì¸¡ì´ ì–´ë ¤ì§€ê³ , ë‚®ìœ¼ë©´ ì‰¬ì›Œì§„ë‹¤.(í™•ë¥ ë¶„í¬ë¥¼ ì¡°ì •)
    "temperature": 1,
    # í™•ë¥ ë¶„í¬ ë‚´ì—ì„œ ì„ íƒí•  ë‹¨ì–´ì˜ ë²”ìœ„ë¥¼ ê²°ì •í•˜ëŠ” ë§¤ê°œë³€ìˆ˜
    "top_p": 0.95,
    # í™•ë¥ ë¶„í¬ ë‚´ì—ì„œ ì„ íƒí•  ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ë§¤ê°œë³€ìˆ˜
    "top_k": 64,
    # ì‘ë‹µí•˜ëŠ” ë©”ì‹œì§€ì˜ ìµœëŒ€ í† í° ìˆ˜
    "max_output_tokens": 8192,
    # ì‘ë‹µí•˜ëŠ” ë©”ì‹œì§€ì˜ ë°ì´í„° íƒ€ì…
    "response_mime_type": "text/plain",
}

# .envì—ì„œ ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

st.title("Gemini-Bot")

@st.cache_resource
def load_model():
    model = genai.GenerativeModel('gemini-1.5-flash',
            generation_config=generation_config)
    print("model loaded...")
    return model

model = load_model()

# streamlitì—ì„œ chatì‚¬ìš©ì‹œ historyê´€ë¦¬ë¥¼ í•´ì¤˜ì•¼ í•œë‹¤.
if "chat_session" not in st.session_state:    
    st.session_state["chat_session"] = model.start_chat(history=[]) 

user_avatar = "C:/Users/park0/gcloud/data/dog.png"
ai_avatar = "ğŸ˜‚"

for content in st.session_state.chat_session.history:
    if content.role == "model":
        with st.chat_message("ai", avatar=ai_avatar):
            st.markdown(content.parts[0].text)
    else:
        with st.chat_message("user", avatar=user_avatar):
            st.markdown(content.parts[0].text)

if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”."):    
    with st.chat_message("user", avatar=user_avatar):
        st.markdown(prompt)
    with st.chat_message("ai", avatar=ai_avatar):
        response = st.session_state.chat_session.send_message(prompt)        
        st.markdown(response.text)